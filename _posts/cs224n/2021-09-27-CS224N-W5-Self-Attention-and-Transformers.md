---
title: CS224N W5. Self attention and Transformer
sidebar:
    nav: cs224n-eng
aside:
    toc: true
key: 20210713
tags: CS224N
---
**All contents is arranged from [CS224N](https://online.stanford.edu/artificial-intelligence/free-content?category=All&course=6097) contents. Please see the details to the [CS224N](https://online.stanford.edu/artificial-intelligence/free-content?category=All&course=6097)!**

## 1. Issue with recurrent models

**$\checkmark$ Linear interaction distance**

RNNs take O(sequence length) steps for distant word pairs to interact.

<p>
    <img src="/assets/images/post/cs224n/w5/intro/issue-recurrent.png" width="400" height="300" class="projects__article__img__center">
    <p align="center">
    <em class="projects__img__caption"> Reference. Stanford CS224n, 2021, Before word 'was', Information ofÂ chefÂ has gone through O(sequence length) many layers!</em>
    </p>
</p>

It's Hard to learn long-distance dependencies because of gradient problems. And linear order of words is â€œbaked inâ€; we already know linear order isnâ€™t the right way to think about sentences.
    
**$\checkmark$  Lack of parallelizability**

## 2. If not recurrence, then what?

### 2.1 Word windows, also known as 1D convolution
    
Word window models aggregate local contexts

<p>
    <img src="/assets/images/post/cs224n/w5/intro/intro-1.png" width="600" height="220" class="projects__article__img__center">
    <p align="center">
    <em class="projects__img__caption"> Reference. Stanford CS224n, 2021, Number of unparallelizable operations does not increase sequence length</em>
    </p>
</p>

- Long-distance dependencies: Stacking word window layers allows interaction between farther words
- Maximum Interaction distanceÂ **= sequence length / window size, but if sequence are too long, you'll just ignore long-distance context**
    
<p>
    <img src="/assets/images/post/cs224n/w5/intro/intro-2.png" width="600" height="220" class="projects__article__img__center">
    <p align="center">
    <em class="projects__img__caption"> Reference. Stanford CS224n, 2021</em>
    </p>
</p>
        
### 2.2 Attention

- Attention treats each word's representation as a query to access and incorporate information from a set of values.
- Number of unparallelizable operations does not increase sequence length.
- Maximum interaction distance: O(1), since all words interact at every layer.
    
<p>
    <img src="/assets/images/post/cs224n/w5/intro/intro-3.png" width="600" height="220" class="projects__article__img__center">
    <p align="center">
    <em class="projects__img__caption"> Reference. Stanford CS224n, 2021</em>
    </p>
</p>

## 3. Self-Attention

- Reference. [https://jalammar.github.io/illustrated-transformer/](https://jalammar.github.io/illustrated-transformer/)

### 3.1 Overview

1. Attention operates onÂ **queries**,Â **keys**, andÂ **values.**
    
    $$
        \begin{aligned}
            &\text{Queries,  } q_1, q_2, \dots,q_T, q_i \in \mathbb{R}^d  \\
            &\text{Keys,  } k_1, k_2, \dots,k_T, k_i \in \mathbb{R}^d     \\
            &\text{Values,  } v_1, v_2, \dots,v_T, v_i \in \mathbb{R}^d   \\
        \end{aligned}
    $$

2. InÂ **self-attention**, the queries, keys, and values are drawn from the same source.
    
    For example, if the output of the previous layer isÂ $ğ‘¥_1, ... , ğ‘¥_ğ‘‡$, (one vec per word) we could letÂ $ğ‘£_ğ‘– = ğ‘˜_ğ‘– = ğ‘_ğ‘– = ğ‘¥_ğ‘–$Â (that is, use the same vectors for all of them!).
    
    In transformer model, it defines,
    
    $$
        ğ‘£_ğ‘– = Vx_i,\ ğ‘˜_ğ‘– = Kx_i,\ ğ‘_ğ‘– = Qx_i,\ K,\ Q,\ V \in \mathbb{R}^{d\times d}
    $$
    
    And we use V and K in encoder for decoder
    
3. The (dot product) self-attention operation is as follows:
    <p>
        <img src="/assets/images/post/cs224n/w5/self-attention/self-attention-equation.png" width="600" height="140" class="projects__article__img__center">
        <p align="center">
        <em class="projects__img__caption"> Reference. Stanford CS224n, 2021</em>
        </p>
    </p>

4. NLP building Block
    <p>
        <img src="/assets/images/post/cs224n/w5/self-attention/self-attention-block.png" width="500" height="400" class="projects__article__img__center">
        <p align="center">
        <em class="projects__img__caption"> Reference. Stanford CS224n, 2021, Self-attention is an operation onÂ sets. It has no inherent notion of order.</em>
        </p>
    </p>

    Self-attention is an operation onÂ **sets**. It has no inherent notion of order.
            
### 3.2 Barriers and solutions for Self-Attention as a building block

**$\checkmark$ 1. Doesn't have an inherent notion for order â†’ Add position representations to the inputs**

$$
    \text{"Sequence order"}
$$

Since self-attention doesnâ€™t build in order information, we need to encode the order of the sentence in our keys, queries, and values. Consider representing eachÂ <span style="background-color: #FCF3CF">**sequence index**</span>Â as aÂ <span style="background-color: #FCF3CF">**vector**</span>

$$
    p_i \in \mathbb{R}^d, \text{for }i \in {1, 2, ..., T}\text{  are position vectors}
$$

Easy to incorporate this info into our self-attention block: just add theÂ ğ‘ğ‘–Â to our inputs. LetÂ $\tilde{v_i},\ \tilde{k_i},\ \tilde{q_i}$Â be our old values, keys, and queries.

$$
    \begin{aligned}
        &v_i=\tilde{v_i}+p_i \\
        &k_i=\tilde{k_i}+p_i \\
        &q_i=\tilde{q_i}+p_i \\
    \end{aligned}
$$

In deep self-attention networks, we do this at the first layer! <span style="background-color: #FCF3CF">**You could concatenate them as well,Â but people mostly just add...**</span>

**Method 1 Sinusoidal position representations.**Â 

- concatenate sinusoidal functions of varying periods:

    $$
        p_i = \begin{bmatrix}
                sin(i/10000^{2*1/d})\\
                cos(i/10000^{2*1/d})\\
                \vdots\\
                sin(i/10000^{2*\frac{d}{2}/d})\\
                cos(i/10000^{2*\frac{d}{2}/d})\\
            \end{bmatrix}
    $$

    <p>
        <img src="/assets/images/post/cs224n/w5/self-attention/self-attention-positioning.png" width="600" height="200" class="projects__article__img__center">
        <p align="center">
        <em class="projects__img__caption"> Reference. Stanford CS224n, 2021</em>
        </p>
    </p>

- Pros
    - Periodicity indicates that maybe â€œabsolute positionâ€ isnâ€™t as important
    - Maybe can extrapolate to longer sequences as periods restart!

- Cons
    - Not learnable
    - The extrapolation doesnâ€™t really work!

**Method 2 Position representation vectors learned from scratch.**

- Learned absolute position representations:Â Let allÂ $p_i$Â be learnable parameters! Learn a matrixÂ $ğ‘ \in \mathbb{R}^{ğ‘‘\times ğ‘‡}$*, and let eachÂ $p_i$*Â *be a column of that matrix*
- Pros: Flexibility, each position gets to be learned to fit the data
- Cons: Definitely canâ€™tÂ extrapolate to indices outsideÂ 1, ... , ğ‘‡.

Most systems use this. Sometimes people try more flexible representations of position:

- Relative linear position attentionÂ [[Shaw et al., 2018]](https://arxiv.org/abs/1803.02155)
- Dependency syntax-based positionÂ [[Wang et al., 2019]](https://arxiv.org/pdf/1909.00383.pdf)

**$\checkmark$ 2. No nonlinearities for deep learning. It's all just weighted averages â†’ adding the same feedforward network to each self-attention output**

- Adding Nonlinearities in self-attention

    Note that there are no elementwise nonlinearities in self-attention; stacking more self-attention layers just re-averagesÂ **value**Â vectors.

    **Add a feed-forward network to post-process each output vector($W_2, b_2$ in below equation)**

    $$
        \begin{aligned}
        m_i &= \text{MLP}(\text{output}_i)\\
            &= W_2 * \text{ReLU}(W_1 \times \text{output}_i + b_1)+b_2
        \end{aligned}
    $$

    <p>
        <img src="/assets/images/post/cs224n/w5/self-attention/self-attention-feedforward.png" width="600" height="400" class="projects__article__img__center">
        <p align="center">
        <em class="projects__img__caption"> Reference. Stanford CS224n, 2021, Intuition: the FF network processes the result of attention</em>
        </p>
    </p>

**$\checkmark$ 3. Need to ensure we donâ€™t â€œlook at the futureâ€ when predicting a sequence â†’ Mask out the future by artificially setting attention weights to 0**

To use self-attention inÂ **decoders**, we need to ensureÂ we canâ€™t peek at the future. To enable parallelization, weÂ **mask out attention**Â to future words by setting attention scores toÂ âˆ’âˆ.

$$
    e_{ij} = \begin{cases}
                q_i^T k_j,  & j<i       \\
                -\infty,    & j \geq i
            \end{cases}
$$

<p>
    <img src="/assets/images/post/cs224n/w5/self-attention/self-attention-not-future.png" width="600" height="500" class="projects__article__img__center">
    <p align="center">
    <em class="projects__img__caption"> Reference. Stanford CS224n, 2021, Intuition: the FF network processes the result of attention</em>
    </p>
</p>

## 4. Transformer model

<p>
    <img src="/assets/images/post/cs224n/w5/transformer/transformer-encoder-decoder.png" width="600" height="300" class="projects__article__img__center">
    <p align="center">
    <em class="projects__img__caption"> Reference. Stanford CS224n, 2021, The Transformer Encoder-DecoderÂ [Vaswani et al., 2017] </em>
    </p>
</p>

### 4.1 Transformer Encoder
**$\checkmark$ Key-query-value attention**: How do we get theÂ ğ‘˜, ğ‘, ğ‘£Â vectors from a single word embedding?

- We saw that self-attention is when keys, queries, and values come from the same source. The Transformer does this in a particular way:
    
    $$
        \text{LetÂ }ğ‘¥_1, \dots , ğ‘¥_ğ‘‡\text{Â be input vectors to the Transformer encoder};Â ğ‘¥_i \in \mathbb{R}^ğ‘‘
    $$

- Then keys, queries, values are:

    $$
        \begin{aligned}
            &k_i = Kx_i\text{, where } K \in \mathbb{R}^{d\times d}\text{ is the key matrix}\\
            &q_i = Qx_i\text{, where } Q \in \mathbb{R}^{d\times d}\text{ is the query matrix}\\
            &v_i = Vx_i\text{, where } V \in \mathbb{R}^{d\times d}\text{ is the value matrix}\\
        \end{aligned}
    $$
        
- These matrices allowÂ *different aspects*Â of theÂ ğ‘¥Â vectors to be used/emphasized in each of the three roles.
- Letâ€™s look at how key-query-value attention is computed, in matrices.
    - LetÂ $ğ‘‹ = [ğ‘¥_1; ... ; ğ‘¥_ğ‘‡] \in \mathbb{R}^{ğ‘‡\times ğ‘‘}$Â be the concatenation of input vectors.
    - First, note thatÂ $ğ‘‹ğ¾ \in \mathbb{R}^{ğ‘‡\times ğ‘‘}, ğ‘‹ğ‘„ \in \mathbb{R}^{ğ‘‡\times ğ‘‘}, ğ‘‹ğ‘‰ \in \mathbb{R}^{ğ‘‡\times ğ‘‘}.$
    - The output is defined asÂ $output = softmax(ğ‘‹ğ‘„(ğ‘‹ğ¾)^âŠ¤)\times ğ‘‹ğ‘‰$
    
    <p>
        <img src="/assets/images/post/cs224n/w5/transformer/transformer-key-query-value.png" width="600" height="250" class="projects__article__img__center">
        <p align="center">
        <em class="projects__img__caption"> Reference. Stanford CS224n, 2021 </em>
        </p>
    </p>
    
    First, take the query-key dot products in one matrix multiplication:Â $ğ‘‹ğ‘„(ğ‘‹ğ¾)^âŠ¤$. Next, softmax, and compute the weighted average with another matrix multiplication.

**$\checkmark$ Multi-headed attention**: Attend to multiple places in a single layer!

- What if we want to look in multiple places in the sentence at once?
    - For wordÂ i, self-attention â€œlooksâ€ where $ğ‘¥_i^âŠ¤ğ‘„^âŠ¤ğ¾ğ‘¥_j$Â Â is high, but maybe we wantÂ to focus on differentÂ jÂ for different reasons?
- Weâ€™ll defineÂ **multiple attention â€œheadsâ€**Â through multiple Q,K,V matrices
- Let, $Q_{\ell}, K_{\ell}, V_{\ell} \in \mathbb{R}^{d \times \frac{d}{h}}$, where h is the number of attention heads, and $\ell$ ranges from 1 to h.
    
    <p>
        <img src="/assets/images/post/cs224n/w5/transformer/transformer-multihead-att.png" width="600" height="200" class="projects__article__img__center">
        <p align="center">
        <em class="projects__img__caption"> Reference. Stanford CS224n, 2021, The Transformer Encoder-DecoderÂ [Vaswani et al., 2017] </em>
        </p>
    </p>
   
- Each attention head performs attention independently:
    
    $$
        \text{output} = \text{softmax}(ğ‘‹ğ‘„_{\ell} ğ¾_{\ell}^âŠ¤ğ‘‹^âŠ¤)*ğ‘‹ğ‘‰_{\ell}\text{Â , whereÂ output}_{\ell} \in R^{\frac{d}{h}}
    $$

- Then the outputs of all the heads are combined!
    
    $$
        \text{output} = ğ‘Œ[\text{output}_1; ... ; \text{output}_h]\text{, whereÂ }ğ‘Œ \in \mathbb{R}^{d\times d}
    $$
    
- Each head gets to â€œlookâ€ at different things, and construct value vectorsÂ differently.

**$\checkmark$ Tricks to help with training!**

1. Residual connections, [[He et al., 2016](https://arxiv.org/abs/1512.03385)]
    
    **Residual connections**Â are a trick to help models train better, [Li et al., 2018](https://arxiv.org/pdf/1712.09913.pdf)

    <p>
        <img src="/assets/images/post/cs224n/w5/transformer/transformer-residual-connection.png" width="600" height="300" class="projects__article__img__center">
        <p align="center">
        <em class="projects__img__caption"> Reference. Stanford CS224n, 2021, Li et al., 2018 </em>
        </p>
    </p>    
    
2. Layer normalization, [[Ba et al., 2016](https://arxiv.org/abs/1607.06450)]
    - **Layer normalization**Â is a trick to help models train faster.
    - Idea: cut down on uninformative variation in hidden vector values by normalizing to unit mean and standard deviationÂ **within each layer**.
    - LayerNormâ€™sÂ success may be due to its normalizing gradients [[Xu et al., 2019](https://papers.nips.cc/paper/2019/file/2f4fe03d77724a7217006e5d16728874-Paper.pdf)]

    $$
        \begin{aligned}
            &\text{Let }Â ğ‘¥ \in \mathbb{R}^ğ‘‘\text{Â be an individual (word) vector in the model.}\\
            &\text{Let }Â \mu = \sum_{j=1}^d x_j;\text{ this is the mean};Â \mu \in \mathbb{R}.\\
            &\text{Let } \sigma = \sqrt{\frac{1}{d}\sum_{j=1}^d(x_j-\mu)^2};\text{ this is the standard deviation}; \sigma \in \mathbb{R}.\\\\
            &\text{Let } \gamma \in \mathbb{R}^d and \beta \in \mathbb{R}^d\text{ be learned "gain" and "bias" parameters (Can omit)}
        \end{aligned}
    $$

    Then layer normalization computes:
    
    $$
        \text{output} = \dfrac{x - \mu}{\sqrt{\sigma}+\epsilon} * \gamma + \beta
    $$

    $$
        \begin{aligned}
            &\sigma,\text{ Normalize by scalar mean and variance}                      \\
            &\gamma, \beta,\text{ Modulate by learned elementwise gain and bias}       \\
        \end{aligned}
    $$

3. Scaling the dot product
    
    **â€œScaled Dot Productâ€**Â attention is a final variation to aid in Transformer training. When dimensionalityÂ ğ‘‘Â becomes large, dot products between vectors tend to become large. Because of this, inputs to the softmax function can be large, making the gradients small.
    
    Instead of the self-attention function weâ€™ve seen:
    
    $$
        output_{\ell} = softmax(XQ_{\ell}K_{\ell}^T X^T)*XV_{\ell}
    $$
    
    We divide the attention scores byÂ $\sqrt{d/h}$, to stop the scores from becoming large just as a function ofÂ $d/h$Â (The dimensionality divided by the number of heads.)
    
    $$
        output_{\ell} = softmax(\dfrac{XQ_{\ell}K_{\ell}^T X^T}{\sqrt{d/h}})*XV_{\ell}
    $$
    
4. These tricksÂ **donâ€™t improve**Â what the model is able to do; they help improve the training process. Both of these types of modeling improvements are very important!

### 4.2 Transformer Encoder-Decoder
        
Looking back at the whole model, zooming in on an Encoder block:

<p>
    <img src="/assets/images/post/cs224n/w5/transformer/transformer-encoder-decoder-1.png" width="600" height="300" class="projects__article__img__center">
    <p align="center">
    <em class="projects__img__caption"> Reference. Stanford CS224n</em>
    </p>
</p>    

Looking back at the whole model, zooming in on a Decoder block:

<p>
    <img src="/assets/images/post/cs224n/w5/transformer/transformer-encoder-decoder-2.png" width="600" height="300" class="projects__article__img__center">
    <p align="center">
    <em class="projects__img__caption"> Reference. Stanford CS224n</em>
    </p>
</p>    

The only new part is attention from decoder to encoder(Multi-Head Cross-Attention and Residual+LayerNorm)

### 4.3 Cross-attention

- We saw that self-attention is when keys, queries, and values come from the same source.
- LetÂ $h_1, \dots , h_ğ‘‡$Â beÂ **output**Â vectorsÂ **from**Â the TransformerÂ **encoder**;Â $ğ‘¥_ğ‘– \in \mathbb{R}^ğ‘‘$
- Let $ğ‘§_1 , \dots , ğ‘§_T$Â be input vectors from the TransformerÂ **decoder**,Â $ğ‘§ \in \mathbb{R}^ğ‘‘$
- Then keys and values are drawn from theÂ **encoder**Â (like a memory):
    
    $$
        k_i=Kh_i,\ v_i=Vh_i
    $$
    
- And the queries are drawn from theÂ **decoder, $q_i=Qz_i$**
- Letâ€™s look at how cross-attention is computed, in matrices.

    $$
        \begin{aligned}
            &\text{LetÂ }H = [h_1; \dots ; h_ğ‘‡] \in \mathbb{R}^{ğ‘‡\times d}\text{Â be the concatenation of encoder vectors.}\\
            &\text{LetÂ }Z = [z_1; \dots ; z_ğ‘‡] \in \mathbb{R}^{ğ‘‡\times d}\text{Â be the concatenation of decoder vectors.}
        \end{aligned}
    $$

    $$
        output = softmax(ğ‘ğ‘„(ğ»ğ¾)^âŠ¤)Ã—ğ»ğ‘‰
    $$

- First, take the query-key dot products in one matrix multiplication:Â $ğ‘ğ‘„(ğ»ğ¾)^âŠ¤$. Next, softmax, and compute the weighted average with another matrix multiplication.
    
<p>
    <img src="/assets/images/post/cs224n/w5/transformer/transformer-cross-att.png" width="600" height="300" class="projects__article__img__center">
    <p align="center">
    <em class="projects__img__caption"> Reference. Stanford CS224n</em>
    </p>
</p>       
    
### 4.4 Recent work
- Quadratic compute in self-attention (today):
    - Computing all pairs of interactions means our computation grows **quadratically**Â with the sequence length!
    - For recurrent models, it only grew linearly!
- Position representations
- Recent work on improving on quadratic self-attention cost
    
    Considerable recent work has gone into the question,Â *Can we build models like Transformers without paying theÂ $ğ‘‚(ğ‘‡^2)$*Â *all-pairs self-attention cost?*
    
    For example,Â **BigBird**Â [[Zaheer et al., 2021](https://arxiv.org/pdf/2006.04768.pdf)]
    
    Key idea: replace all-pairs interactions with a family of other interactions,Â **like local windows**,Â **looking at everything**, andÂ **random interactions**.

    <p>
        <img src="/assets/images/post/cs224n/w5/transformer/transformer-recent.png" width="600" height="150" class="projects__article__img__center">
        <p align="center">
        <em class="projects__img__caption"> Reference. Stanford CS224n</em>
        </p>
    </p>    


## 5. Pre-training

### 5.1 Brief overview

**$\checkmark$ Word structure and subword models**

Letâ€™sÂ take a lookÂ at the assumptions weâ€™ve made about a languageâ€™s vocabulary. We assume a fixed vocab of tens of thousands of words, built from the training set. AllÂ *novel*Â words seen at test time are mapped to a single UNK.

<p>
    <img src="/assets/images/post/cs224n/w5/pretraining/pretraining-word-to-vocab.png" width="600" height="250" class="projects__article__img__center">
    <p align="center">
    <em class="projects__img__caption"> Reference. Stanford CS224n</em>
    </p>
</p>    

Finite vocabulary assumptions make evenÂ *less*Â sense in many languages.Â Many languages exhibit complexÂ **morphology**, or word structure.

- The effect is more word types, each occurring fewer times.

Example: Swahili verbs can have hundreds of conjugations, each encoding a wide variety of information. (Tense, mood, definiteness, negation, information about the object, ++)

**$\checkmark$ The byte-pair encoding algorithm**
    
Subword modeling in NLP encompasses a wide range of methods for reasoning about structure below the word level. (Parts of words, characters, bytes.)

- The dominant modern paradigm is to learn a vocabulary ofÂ **parts of words (subword tokens).**
- At training and testing time, each word is split into a sequence of known subwords.
    
    Byte-pair encodingÂ is a simple, effective strategy for defining a subword vocabulary.
    
    1. Start with aÂ vocabulary containing only characters and an â€œend-of-wordâ€ symbol.
    2. Using a corpus of text, find the mostÂ common adjacent characters â€œa,bâ€; add â€œabâ€ as aÂ subword.
    3. Replace instances of the character pair with the new subword; repeat until desired vocab size.
    
    Originally used in NLP for machine translation; now a similar method (WordPiece) is used in pretrained models.
        

Common words end up being a part of the subword vocabulary, while rarer words are split into (sometimes intuitive, sometimes not) components.

In the worst case, words are split into as many subwords as they have characters.

<p>
    <img src="/assets/images/post/cs224n/w5/pretraining/pretraining-word-to-vocab-2.png" width="600" height="250" class="projects__article__img__center">
    <p align="center">
    <em class="projects__img__caption"> Reference. Stanford CS224n</em>
    </p>
</p> 

**$\checkmark$  Motivating model pretraining from word embeddings**
    
Recall the adage we mentioned at the beginning of the course:

    â€œYou shall know a word by the company it keepsâ€Â (J. R. Firth 1957: 11)

This quote is a summary ofÂ **distributional semantics**, and motivatedÂ **word2vec**. But:

    â€œ... the complete meaning of a word is always contextual,
    
    and no study of meaning apart from a complete contextÂ 
    
    can be taken seriously.â€Â (J. R. Firth 1935)

ConsiderÂ IÂ **record**Â theÂ **record***: the two instances ofÂ **record**Â mean different things.
    
**$\checkmark$ Where we were:Â pretrained word embeddings, Circa 2017**

- Start with pretrained word embeddings (no context!)
- Learn how to incorporate context in an LSTM or Transformer while training on the task.

<p>
    <img src="/assets/images/post/cs224n/w5/pretraining/pretraining-word-embedding.png" width="600" height="400" class="projects__article__img__center">
    <p align="center">
    <em class="projects__img__caption"> Reference. Stanford CS224n, Recall,Â movieÂ gets the same word embedding, no matter what sentence it shows up in</em>
    </p>
</p> 

**$\checkmark$ Some issues to think about:**

1. The training data we have for ourÂ **downstream task**Â (like question answering) must be sufficient to teach all contextual aspects of language.
2. Most of the parameters in our network are randomly initialized!

In modern NLP:

- All (or almost all) parameters in NLP networks are initialized viaÂ **pretraining**.
- Pretraining methods hide parts of the input from the model, and train the model to reconstruct those parts.
    
<p>
    <img src="/assets/images/post/cs224n/w5/pretraining/pretraining-word-embedding-joint.png" width="600" height="400" class="projects__article__img__center">
    <p align="center">
    <em class="projects__img__caption"> Reference. Stanford CS224n, This model has learned how to represent entire sentences through pretraining </em>
    </p>
</p> 

- This has been exceptionally effective at building strong:
    - **representations of language**
    - **parameter initializations**Â for strong NLP models.
    - **Probability distributions**Â over language that we can sample from

Recall theÂ **language modeling**Â task:

- ModelÂ $p_{\theta}(w_t\lvert w_{1:t-1})$, the probability distribution over words given their past contexts.
    
    <p>
        <img src="/assets/images/post/cs224n/w5/pretraining/pretraining-language-model.png" width="600" height="400" class="projects__article__img__center">
        <p align="center">
        <em class="projects__img__caption"> Reference. Stanford CS224n, Step 1: Pretrain (on language modeling) </em>
        </p>
    </p>     

**$\checkmark$ Pretraining through language modelingÂ [[Dai and Le, 2015](https://arxiv.org/pdf/1511.01432.pdf)]**

- Train a neural network to perform language modeling on a large amount of text.
- Save the network parameters.

Pretraining can improve NLP applications by serving as parameter initialization.

- Step 1: Pretrain (on language modeling): Lots of text; learn general things!

- Step 2: Finetune (on your task) Not many labels; adapt to the task!

<p>
    <img src="/assets/images/post/cs224n/w5/pretraining/pretraining-language-model-finetune.png" width="600" height="400" class="projects__article__img__center">
    <p align="center">
    <em class="projects__img__caption"> Reference. Stanford CS224n, Step 1: Pretrain (on language modeling) </em>
    </p>
</p>     

Why should pretraining and finetuning help, from a â€œtraining neural netsâ€ perspective? Consider, provides parameters $\hat \theta$ by approximating $\underset{\theta}{min}\ L_{pretrain}(\theta)$, pretrain loss. Then, finetuning approximates $\underset{\theta}{min}\ L_{finetune}(\theta)$, starting at $\hat \theta$, finetuning loss. The pretraining may matter because stochastic gradient descent sticks (relatively) close to $\hat \theta$ during finetuning.

- So, maybe the finetuning local minima nearÂ $\hat \theta$Â tend to generalize well!
- And/or, maybe the gradients of finetuning loss nearÂ $\hat \theta$Â propagate nicely!

### 5.2 Model Pretraining ways

<p>
    <img src="/assets/images/post/cs224n/w5/pretraining/pretraining-types.png" width="600" height="300" class="projects__article__img__center">
    <p align="center">
    <em class="projects__img__caption"> Reference. Stanford CS224n, 2021</em>
    </p>
</p>

**$\checkmark$ #1 Decoders**
    
1. **Pretraining**

    When using language model pretrained decoders, we can ignore that they were trained to modelÂ $p(w_t\lvert w_{1:t-1})$. We can fine-tune them by training a classifier on the lsat word's hidden state.

    <p>
        <img src="/assets/images/post/cs224n/w5/pretraining/pretraining-decoder.png" width="600" height="450" class="projects__article__img__center">
        <p align="center">
        <em class="projects__img__caption"> Reference. Stanford CS224n, 2021, Note how the linear layer hasnâ€™t beenÂ pretrained and must be learned from scratch.</em>
        </p>
    </p>

    $$
        h_1, \dots, h_T = Decoder(w_1,\dots, w_T) \\
        y\sim Aw_T +b   
    $$

    WhereÂ AÂ andÂ bÂ are randomly initialized and specified by the downstream task. Gradients backpropagate through the whole network.

2. **Finetuning**

    <p>
        <img src="/assets/images/post/cs224n/w5/pretraining/pretraining-decoder-finetune.png" width="600" height="450" class="projects__article__img__center">
        <p align="center">
        <em class="projects__img__caption"> Reference. Stanford CS224n, 2021, Note how the linear layer has been pretrained.</em>
        </p>
    </p>

    Itâ€™s natural to pretrain decoders as language models and then use them as generators, finetuning their $p_{\theta}(w_t\lvert w_{1:t-1})$.

    This is helpful in tasksÂ **where the output is a sequence**Â with a vocabulary like that at pretraining time. 

    - Dialogue (context=dialogue history)
    - Summarization (context=document)

    $$
        h_1, \dots, h_T = Decoder(w_1,\dots, w_T) \\
        w_t \sim Aw_{t-1} +b
    $$

    WhereÂ A, bÂ were pretrained in the language model

**$\checkmark$ Generative Pretrained Transformer (GPT) [[Radford et al., 2018]](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf)**

2018â€™s GPT wasÂ a big success in pretraining a decoder!

- Transformer decoder with 12 layers.
- 768-dimensional hidden states, 3072-dimensional feed-forward hidden layers.
- Byte-pair encoding with 40,000 merges
- Trained on BooksCorpus: over 7000 unique books.
    
    - Contains long spans of contiguous text, for learning long-distance dependencies.

- The acronym â€œGPTâ€Â never showed up in the original paper; it could stand for â€œGenerativeÂ PreTrainingâ€ or â€œGenerative PreTrained Transformerâ€
- How do we format inputs to our decoder forÂ **finetuning tasks?**
    
    - [Radford et al., 2018]((https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf)) evaluate on natural language inference.

    - Natural Language Inference:Â Label pairs of sentences as *entailing/contradictory/neutral*
    
    - Entailment
    
        ```
            Premise:Â The man is in the doorwayÂ 
            Hypothesis:Â The person is near the door
        ```    
    Hereâ€™s roughly how the input was formatted, as a sequence of tokens for the decoder.
    
    ```
        [START]Â The man is in the doorwayÂ [DELIM]Â The person is near the doorÂ [EXTRACT]
    ```

    The linear classifier is applied to the representation of the [EXTRACT] token.
    
- Increasingly convincing generations (GPT2) [[Radford et al., 2018](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf)]
    
    We mentioned how pretrained decoders can be usedÂ **in their capacities as language models.**
    
    **GPT-2,**Â a larger version of GPT trained on more data, was shown to produce relatively convincing samples of natural language.
        
**$\checkmark$ #2 Encoders, Encoders get bidirectional context!**
    
<p>
    <img src="/assets/images/post/cs224n/w5/pretraining/pretraining-encoder.png" width="600" height="400" class="projects__article__img__center">
    <p align="center">
    <em class="projects__img__caption"> Reference. Devlin et al., 2018</em>
    </p>
</p>

Idea: **replace some fraction of words in the input with a special [MASK] token**; predict these words.

$$
    h_1,\dots,h_T = Encoder(w_1,\dots, w_T) \\
    y_i\sim Aw_i+b
$$

Only add loss terms from words that areÂ â€œmasked out.â€ IfÂ $\tilde x$Â is the masked version ofÂ x,Â weâ€™re learningÂ $p_{\theta}(x\lvert \tilde x)$. CalledÂ **Masked LM**.

**$\checkmark$ BERT: Bidirectional Encoder Represetations from Transformers**

[Devlin et al., 2018](https://arxiv.org/pdf/1810.04805.pdf) proposed the â€œMasked LMâ€ objective andÂ **released the weights of a pretrained Transformer**, a model they labeled BERT.

Some more details about Masked LM for BERT:

<p>
    <img src="/assets/images/post/cs224n/w5/pretraining/pretraining-bert.png" width="600" height="400" class="projects__article__img__center">
    <p align="center">
    <em class="projects__img__caption"> Reference. Devlin et al., 2018</em>
    </p>
</p>

- Predict a random 15% of (sub)word tokens.
    - Replace input word with [MASK] 80% of the time
    - Replace input word with a random token 10% of the time
    - Leave input word unchanged 10% of the time (but still predict it!)
- Why?Â Doesnâ€™tÂ let the model get complacent and not build strong representations of non-masked words. (No masks are seen at fine-tuning time!)
- The Pretraining input to BERT was two separate contiguous chunks of text: "Segment Embedding", [Devlin et al., 2018](https://arxiv.org/pdf/1810.04805.pdf),Â [Liu et al., 2019](https://arxiv.org/pdf/1907.11692.pdf)]
    
    <p>
        <img src="/assets/images/post/cs224n/w5/pretraining/pretraining-bert-segment-embedding.png" width="600" height="250" class="projects__article__img__center">
        <p align="center">
        <em class="projects__img__caption"> Reference. Devlin et al., 2018, Liu et al., 2019</em>
        </p>
    </p>
    
- BERT was trained to predict whether one chunk follows the other or is randomly sampled. Later work hasÂ argued this â€œnext sentence predictionâ€ is not necessary.

- Detail
    - Two models were released:
        
        BERT-base: 12 layers, 768-dim hidden states, 12 attention heads, 110 million params
        
        BERT-large: 24 layers, 1024-dim hidden states, 16 attention heads, 340 million params.
        
    - Trained on:
        
        BooksCorpus (800 million words)
        
        English Wikipedia (2,500 million words)
        
    - Pretraining is expensive and impractical on a single GPU.
        
        BERT was pretrained with 64 TPU chips for a total of 4 days.Â 
        
        TPUs are special tensor operation acceleration hardware)
        
    - Finetuning is practical and common on a single GPU
        
        â€œPretrainÂ once, finetune manyÂ times.â€
        
    - Evaluation
        - **QQP:**Â Quora Question Pairs (detect paraphraseÂ â€¢Â questions)
        - **QNLI**: natural language inference over question answering data
        - **SST-2**: sentiment analysis
        - **CoLA**: corpus of linguistic acceptability (detect whether sentences are grammatical.)
        - **STS-B**: semantic textual similarity
        - **MRPC**: microsoft paraphrase corpus
        - **RTE**: a small natural language inference corpus
        
- Limitness
    
    If your task involves generating sequences, consider using a pretrained decoder; BERT and otherÂ pretrained encoders donâ€™t naturally lead to nice autoregressive (1-word-at-a-time) generation methods.
    
    <p>
        <img src="/assets/images/post/cs224n/w5/pretraining/pretraining-bert-limitness.png" width="600" height="200" class="projects__article__img__center">
        <p align="center">
        <em class="projects__img__caption"> Reference. Stanford CS224n</em>
        </p>
    </p>
    
- Expansions of BERT
    
    RoBERTa, SpanBERT, +++
    
    1. RoBERTa: mainly just train BERT for longer and remove next sentence prediction!, [Liu et al., 2019](https://arxiv.org/pdf/1907.11692.pdf)
        
        <p>
            <img src="/assets/images/post/cs224n/w5/pretraining/pretraining-roberta.png" width="600" height="300" class="projects__article__img__center">
            <p align="center">
            <em class="projects__img__caption"> Reference. Liu et al., 2019</em>
            </p>
        </p>
        
    2. SpanBERT: masking contiguous spans of words makes a harder, more useful pretraining task, [Joshi et al., 2020](https://arxiv.org/abs/1907.10529)
        
        <p>
            <img src="/assets/images/post/cs224n/w5/pretraining/pretraining-spanbert.png" width="600" height="300" class="projects__article__img__center">
            <p align="center">
            <em class="projects__img__caption"> Reference. Joshi et al., 2020</em>
            </p>
        </p>     
        
**$\checkmark$#3 Encoder-Decoders**
    
ForÂ **encoder-decoders**, we could do something likeÂ **language modeling**, but where a prefix of every input is provided to the encoder and is not predicted.

<p>
    <img src="/assets/images/post/cs224n/w5/pretraining/pretraining-encoder-decoder.png" width="500" height="450" class="projects__article__img__center">
    <p align="center">
    <em class="projects__img__caption"> Reference. Raffel et al., 2018</em>
    </p>
</p>

$$
    h_1,\dots,h_T = Encoder(w_1,\dots, w_T)\\
    h_{T+1},\dots,h_{2T} = Decoder(w_1,\dots, w_T,\sim h_1,\dots, h_T)\\
    y_i\sim Aw_i+b, i>T\\
$$

TheÂ **encoder**Â portion benefits from bidirectional context; theÂ **decoder**Â portion is used to train the whole model through language modeling.

WhatÂ [Raffel et al., 2018](https://arxiv.org/pdf/1910.10683.pdf)Â found to work best wasÂ **span corruption.**Â Their model:Â **T5** , which replaces different-length spans from the input with unique placeholders; decode out the spans that were removed

<p>
    <img src="/assets/images/post/cs224n/w5/pretraining/pretraining-T5-1.png" width="400" height="70" class="projects__article__img__center">
    <p align="center">
    </p>
</p>

<p>
    <img src="/assets/images/post/cs224n/w5/pretraining/pretraining-T5-2.png" width="600" height="350" class="projects__article__img__center">
    <p align="center">
    <em class="projects__img__caption"> Reference. Raffel et al., 2018</em>
    </p>
</p>

This is implemented in textÂ preprocessing: itâ€™s still an objectiveÂ that looks likeÂ **language modeling**Â at the decoder side.

A fascinating property of T5: it can be finetuned to answer a wide range of questions, retrieving knowledge from its parameters.

<p>
    <img src="/assets/images/post/cs224n/w5/pretraining/pretraining-T5-property.png" width="600" height="300" class="projects__article__img__center">
    <p align="center">
    <em class="projects__img__caption"> Reference. Raffel et al., 2018, NQ: Natural Questions, WQ: WebQuestions, TQA: Trivia QA, All "open-domain" versions</em>
    </p>
</p>

## 6. GPT-3, **In-context learning, and very large models**

Very large language models seem to perform some kind of learningÂ without gradient stepsÂ simply from examples you provide within their contexts. GPT-3 is the canonical example of this. **The largest T5 model had 11 billion parameters. GPT-3 has 175 billion parameters.**

The in-context examples seem to specify the task to be performed, and the conditional distribution mocks performing the task to a certain extent.

**Input (prefix within a single Transformer decoder context):**

    " thanks â†’ merci

        hello â†’ bonjour 

        mint â†’ menthe

        otter â†’               " 

**Output (conditional generations):**

    loutre..."

Very large language models seem to perform some kind of learningÂ **without gradient steps**Â simply from examples you provide within their contexts.

<p>
    <img src="/assets/images/post/cs224n/w5/pretraining/pretraining-GPT3.png" width="600" height="300" class="projects__article__img__center">
    <p align="center">
    <em class="projects__img__caption"> Reference. Stanford CS224n </em>
    </p>
</p>
